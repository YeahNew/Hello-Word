DRIVER_OPTIONS="--master spark://10.42.101.70:7078 --total-executor-cores 48 --conf spark.driver.memory=100g --conf spark.default.parallelism=100 --conf spark.sql.adaptive.enabled=false --driver-jav    a-options -Dlog4j.configuration=file:///${OUTPUT_DIR}/log4j.properties"
EXECUTOR_OPTIONS="--executor-cores 48 --conf spark.executor.memory=80g --conf spark.task.cpus=1 --conf spark.sql.shuffle.partitions=100 --conf spark.executor.extraJavaOptions=-Dlog4j.configuration=f    ile:///${OUTPUT_DIR}/log4j.properties --conf spark.sql.crossJoin.enabled=true"

--------------
select c_last_name from store_sales, customer where store_sales.ss_customer_sk = customer.c_customer_sk limit 100;

select count(*) from (select c_last_name from store_sales, customer where store_sales.ss_customer_sk = customer.c_customer_sk) hot_cust limit 100;

DRIVER_OPTIONS="--master spark://10.42.101.70:7078 --jars ${CUDF_JAR},${RAPIDS_JAR} \ 
--conf spark.executor.extraClassPath=${CUDF_JAR},${RAPIDS_JAR} \
--conf spark.driver.extraClassPath=${CUDF_JAR},${RAPIDS_JAR} \
--conf spark.plugins=com.nvidia.spark.SQLPlugin \
--conf spark.rapids.sql.incompatibleOps.enabled=true \
--conf spark.rapids.memory.gpu.pooling.enabled=true \
--conf spark.shuffle.service.enabled=false \
--conf spark.rapids.shuffle.transport.enabled=true \
--conf spark.rapids.shuffle.manager.enabled=true \
--conf spark.executor.resource.gpu.amount=1 \
--conf spark.rapids.memory.gpu.allocFraction=0.9 \
--conf spark.rapids.shuffle.transport.enabled=true \
--conf spark.rapids.sql.enabled=true \
--conf spark.rapids.sql.concurrentGpuTasks=4 \
--conf spark.task.resource.gpu.amount=0.125 \
--conf spark.driver.maxResultSize=60g \
--conf spark.locality.wait=0s \
--conf spark.rapids.sql.hasNans=true \
--conf spark.executor.extraJavaOptions="-Dai.rapids.cudf.prefer-pinned=true" \ 
--total-executor-cores 24 \
--driver-memory 50g \
--conf spark.default.parallelism=30 \
--conf spark.sql.adaptive.enabled=false \
--driver-java-options -Dlog4j.configuration=file:///${OUTPUT_DIR}/log4j.properties"

EXECUTOR_OPTIONS="--executor-cores 8 \
--conf spark.executor.memory=50g \
--conf spark.task.cpus=1 \
--conf spark.sql.shuffle.partitions=30 \
--conf spark.executor.extraJavaOptions=-Dlog4j.configuration=file:///${OUTPUT_DIR}/log4j.properties \
--conf spark.sql.crossJoin.enabled=true"

